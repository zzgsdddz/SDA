---
title: "SDA Group Submission Assignment Assign7"
subtitle: "Group Gr18" 
author: "MengliFeng (2720589) and PepijnVanOostveen (2801582)"
output: 
  pdf_document:
    keep_tex: true
header-includes:
  - \usepackage{fvextra}
  - \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
---

# Exercise 1
```{r}
# Load required package
library(openintro)

# View dataset structure
str(mammals)
mammals[c(1,5),]$species
```

## a.
```{r}
# Fit the linear model
model_full <- lm(brain_wt ~ body_wt + gestation, data = mammals)

# Model summary
summary(model_full)

# Default diagnostic plots (2 x 2 grid)
par(mfrow = c(2, 2), mar = c(4, 4, 2, 2))
plot(model_full)
```

	
	
## b.
In a well-fitting linear regression model:
	•	Residuals should be randomly scattered around the horizontal line at 0,
	•	There should be no visible patterns,
	•	The spread (variance) of residuals should be roughly constant across fitted values.

In the first plot:
	•	The residuals deviate sharply from 0 at higher fitted values,
	•	There is a curved or fan-shaped pattern — suggesting non-linearity,
	•	Points 1, 5, and 34 are far away from the bulk of other residuals, affecting the pattern.
Therefore, the linearity assumption is not satisfied

In Q-Q Plot:
	•	The points deviate notably from the theoretical line, especially at the extremes (left and right tails).
This indicates that the residuals are not normally distributed, violating the normality assumption.

In Scale-Location Plot:
	•	The spread (variance) of the standardized residuals increases dramatically for larger fitted values.
	•	The line is not flat, and there’s a strong trend upward, indicating heteroscedasticity (non-constant variance).
	
In Residual vs leverage plot:
point 1 and 5 have Cook’s distance larger than 1 and can thus be identified as influence points: they are Africanelephant Asianelephant 
	
## c.
```{r}
# Regression 1: brain_wt ~ body_wt
model_body <- lm(brain_wt ~ body_wt, data = mammals)

# Regression 2: brain_wt ~ gestation
model_gest <- lm(brain_wt ~ gestation, data = mammals)

# Plot both with regression lines
par(mfrow = c(1, 2))

plot(mammals$body_wt, mammals$brain_wt,
     main = "brain_wt vs body_wt", xlab = "body_wt", ylab = "brain_wt")
abline(model_body, col = "blue")

plot(mammals$gestation, mammals$brain_wt,
     main = "brain_wt vs gestation", xlab = "gestation", ylab = "brain_wt")
abline(model_gest, col = "blue")
```
Left Plot: brain_wt vs body_wt
	•	There is a strong positive relationship between body weight and brain weight.
	•	However, the relationship appears to be nonlinear and heavily influenced by a few large outliers, particularly two points with very large body and brain weights.
	•	Most of the data are clustered tightly near the origin, making it difficult to see detailed trends without a transformation.

Right Plot: brain_wt vs gestation
	•	There is also a positive trend between gestation time and brain weight, though the correlation is weaker than with body weight.
	•	The spread of points is again highly skewed, with most values clustered near the bottom-left corner and a few large outliers pulling the regression line upward.
	•	The linear fit line again does not capture the trend well for the majority of data, likely due to skewness and nonlinearity.
	
## d.
```{r}
# Log-log model
model_log <- lm(log(brain_wt) ~ log(body_wt) + log(gestation), data = mammals)

# Summary
summary(model_log)

# Diagnostic plots for log-log model
par(mfrow = c(2, 2), mar = c(4, 4, 2, 2))
plot(model_log)

# Cook's distance for log-log model
cooksD_log <- cooks.distance(model_log)
which(cooksD_log > 1)
```
Linearity Assumption (Residuals vs Fitted Plot)
	•	The residuals are now more randomly scattered around 0, with no obvious non-linear trend.
	•	This suggests the linearity assumption is much better satisfied than in the untransformed model.
Normality of Residuals (Q-Q Plot)
	•	Points lie much closer to the diagonal line, especially in the center.
	•	While a few outliers (e.g., obs. 34 and 500) remain in the tails, overall normality is significantly improved.
Homoscedasticity (Scale-Location Plot)
	•	The spread of standardized residuals is much more uniform across the range of fitted values.
	•	The red trend line is relatively flat, indicating constant variance — a key assumption now better met.
Influence Points (Residuals vs Leverage Plot)
	•	Influential points are still present (e.g., 34, 61), but they do not exceed Cook’s distance threshold.
	•	Leverage values are low overall, and no observation is as extreme as in the original model.

# Exercise 2
## a.
```{R}
set.seed(123)
x1 <- runif(100, min = 0, max = 10)
x2 <- runif(100, min = 0, max = 5)
y <- 7 + 0.8 * x1 + 0.4 * x2 + rnorm(100, mean = 0, sd = 1.5)
```
This code corresponds to the linear model:
$$y=\beta_0 + \beta_1x_1 + \beta_2x_2 + \epsilon$$
thus:
$$\beta_0 = 7,\ \beta_1=0.8,\ \beta_2=0.4$$
and $\epsilon$ is the error which is normally distributed ($\mathcal{N}(0,1.5^2)$).

```{r}
model <- lm(y ~ x1 + x2)
summary(model)
```
The estimated values are close to the actual values. The estimates for $\beta_0,\  \beta_1$ and $\beta_2$ are $6.79$, $0.81$ and $0.39$ respectively.  
Both $H_0:\beta_1=0$ and $H_0:\beta_2=0$ would be rejected at $\alpha = 5\%$ since the p-values are $<2\cdot10^{-16}$ and $0.000727$ respectively.
## b.
```{R}
set.seed(123)
x1 <- runif(100, min = 0, max = 10)
x2 <- 0.5 * x1 + rnorm(100, sd = 0.1)
y <- 7 + 0.8 * x1 + 0.4 * x2 + rnorm(100, mean = 0, sd = 1.5)
```
We again have the same linear model:
$$y=\beta_0 + \beta_1x_1 + \beta_2x_2 + \epsilon$$
And thus the same values for the $\beta_i$ and error:
$$\beta_0 = 7,\ \beta_1=0.8,\ \beta_2=0.4,\ \epsilon\sim\mathcal{N}(0,1.5^2)$$
The only thing that $x_2$ is now correlated to $x_1$ since:
$$x_2=0.5x_1 + error$$
where $error\sim\mathcal{N}(0,0.1^2)$ is relatively small.

```{r}
model <- lm(y ~ x1 + x2)
summary(model)
```
The estimates aren't close to the actual values anymore, except for the intercept which is even a little closer to the actual value than before. (The estimates for $\beta_0,\  \beta_1$ and $\beta_2$ are $7.10$, $1.42$ and $-0.88$ respectively.)
Both $H_0:\beta_1=0$ and $H_0:\beta_2=0$ now fail to be rejected at $\alpha = 5\%$ although the first was still close to being rejected since the p-values are $0.0563$ and $0.5523$ respectively.

In $(a)$ $x_1$ and $x_2$ were independent, but now there is a high correlation between the two. This causes multicollinearity which increases the variance of estimators. The correlation makes it difficult to find the individual effect of each predictor.

## c.
We don't need to fit the regression explicitly since we can use the squared correlation:
$$R_i^2=r_i^2$$
where r is the correlation between the two variables.

```{r}
r1 <- cor(x1, x2)
VIF1 <- 1 / (1 - r1^2)

r2 <- cor(x2, x1)
VIF2 <- 1 / (1 - r2^2)

VIF1
VIF2
```
High single digit $VIF$ already indicate high multicollinearity so $VIF_1=VIF_2 = 218.5474$ shows a very high multicollinearity between $x1$ and $x_2$ as we already concluded in $(b)$.

$VIF_1=VIF_2$ since when we regress $x_1$ on $x_2$ or $x_2$ on $x_1$ with just 2 predictors $R^2_i$ is equal to the correlation coefficient $r$ squared and the correlation between two variables is symmetric.
